项目开发说明文档

一、项目总体定位（Project Vision） 

本项目旨在探索一种面向社区与机构养老场景的多模态交互式陪伴机器人原型，通过投影式认知交互 + 非接触健康监测 + 情感陪伴机制的有机融合，回应当前养老机构中：

老人训练依从性低 健康监测侵入性强、割裂于日常生活 陪伴系统缺乏情感连续性与个体理解 等核心问题。 项目并非单一功能设备，而是一个以“状态感知”为核心的交互系统原型。

二、核心设计理念（Core Idea）

三个核心能力不是并列，而是联动的 本项目并不将“认知训练 / 健康监测 / 情感陪伴”视为三条独立功能线，而是构建如下关系： 状态感知 -> 动态判断 -> 交互调节

健康监测提供身体状态线索

情绪与行为识别提供心理与参与状态线索

认知交互系统根据状态进行强度、节奏、内容的自适应调整

投影交互是核心交互媒介，而非附属展示 与传统屏幕或可穿戴设备不同，本项目选择投影式交互作为主要认知训练载体，原因在于：

降低设备操作门槛

更适合群体空间 / 公共活动区域

可自然融合身体运动与认知任务

为“机器人存在感”提供空间层面的表达 投影并非单纯“显示”，而是构成认知任务本身的一部分。

三、系统整体结构（Conceptual Architecture） 

从概念上，系统分为四个层级：

[ 感知层 ]

摄像头（RGB）

姿态 / 行为线索

生理信号线索（如 rPPG）

[ 状态理解层 ]

健康状态估计

情绪 / 参与度判断

异常行为识别

[ 交互决策层 ]

认知训练强度调节

任务节奏与反馈方式调整

陪伴策略选择

[ 表达与交互层 ]

地面/桌面投影认知游戏

声音 / 语音反馈

机器人行为与提示

备注：各层之间是松耦合关系，算法与实现方式允许在后期不断替换与演进。

四、健康监测方向（概念性说明） 

健康监测在本项目中不是医疗级诊断工具，而是： “用于判断老人当下是否适合继续参与交互活动”的状态参考系统。 可能采用的感知线索包括（不限定）：

基于 RGB 摄像头的 rPPG 心率估计

基于视觉的姿态与活动水平判断

行为节律变化（如停顿、反应迟缓） 核心作用：

发现明显异常状态（疲劳、状态波动）

作为交互调节的输入信号

为护理人员提供参考线索（而非结论）

五、情感陪伴方向（概念性说明） 

情感陪伴并非“聊天机器人”，而是强调： 长期互动中形成的情感连续性与熟悉感。 关键设计点：

记忆能力：记录用户偏好、过往互动、训练历史，形成“被记住”的体验。

情绪与状态感知：利用视觉与行为线索推测参与状态。不追求高精度情绪标签，而是“趋势判断”。

融入认知训练：情感反馈不独立存在，而是体现在任务难度变化、反馈语气、游戏内容选择。

六、认知训练的自适应逻辑（核心创新点） 

本项目中的认知训练并非固定关卡式，而是： 基于老人实时状态进行动态调整的交互过程。 自适应可能体现在：

训练强度（速度、复杂度）

任务持续时间

反馈频率与形式

是否引入身体运动元素 其判断依据来自：

生理信号趋势（如 rPPG）

行为与参与度

过往训练表现与记忆数据

七、项目阶段性产出（Project Deliverables） 

阶段一（原型验证）：

可运行的投影交互原型（或模拟）

基础健康状态感知示例（如 rPPG）

前后端联通的实时系统

阶段二（交互深化）：

至少一个完整的投影认知训练场景

简单的状态驱动调节逻辑

情感反馈与记忆概念验证

最终产出形式：

可演示的系统级交互原型

配套说明视频 / 场景演示

展览空间中的实体或半实体呈现

要求1.局域网调用平板的摄像头，来做rppg和情绪识别还有姿式检测。2.调用电脑的摄像头，之后会考虑电脑外接一个摄像头，现在先用电脑自带的摄像头，来检测人与地面上投影界面的交互。UI应该是分为两部分，一部分是能通过局域网展示在平板上，作为机器人的UI系统。另一部分是经过投影仪，投影到地面的内容，如认知训练游戏。4.这些是联动的，比如我在平板网页上点开始认知训练，就能将认知训练网页投影到地上。当我做认知训练的时候，平板上就显示实时的正确率、心率、强度、训练效果等。5.我们需要调整文件结构，把不必要的py和md删掉，后续你做测试的时候产生的md和py也要明显的告诉我这是一个测试文件，可以删除。
